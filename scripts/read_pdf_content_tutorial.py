import json
from ast import literal_eval
from os.path import join

import pandas as pd

from scripts.read_pdf_files import get_pdf_content_from_url

# directory to csv generated by scrap_pdf_files.py

data_directory = "../data"
csv_name = "aides_v2.csv"

path_to_csv = join(data_directory, csv_name)
data = pd.read_csv(path_to_csv)

data_with_pdf = data[data['pdfs'] != '[]']

aide = data_with_pdf.sample().iloc[0]  # aide prise au hasard
aide_name = aide['name']  # nom de l'aide
pdfs_list = literal_eval(aide['pdfs'])  # liste des urls des pdfs associés à l'aide

content_dict = {}
for url in pdfs_list:
    content = get_pdf_content_from_url(url)
    content_dict[url] = content

path = join(data_directory, aide_name + '.json')
with open(path, "w") as f:
    json.dump(content_dict, f)
